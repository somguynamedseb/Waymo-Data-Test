{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBFvXsCOxHeV"
      },
      "source": [
        "#  Vision-based End-to-End Driving Tutorial\n",
        "\n",
        "- Website: https://waymo.com/open\n",
        "- GitHub: https://github.com/waymo-research/waymo-open-dataset\n",
        "- Challenge: https://waymo.com/open/challenges/2025/e2e-driving/\n",
        "\n",
        "This tutorial demonstrates how to load, visualize and submit end-to-end driving data. Visit the [Waymo Open Dataset Website](https://waymo.com/open) to download the full dataset.\n",
        "\n",
        "To use, open this notebook in [Colab](https://colab.research.google.com).\n",
        "\n",
        "Uncheck the box \"Reset all runtimes before running\" if you run this colab directly from the remote kernel. Alternatively, you can make a copy before trying to run it by following \"File > Save copy in Drive ...\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBYit3Qhxw3E"
      },
      "source": [
        "## Package installation üõ†Ô∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOGwtE4EyZmY"
      },
      "outputs": [],
      "source": [
        "!pip install waymo-open-dataset-tf-2-12-0==1.6.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBAbHAxuwmic"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import cv2\n",
        "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
        "from waymo_open_dataset.wdl_limited.camera.ops import py_camera_model_ops\n",
        "\n",
        "from waymo_open_dataset.protos import end_to_end_driving_data_pb2 as wod_e2ed_pb2\n",
        "from waymo_open_dataset.protos import end_to_end_driving_submission_pb2 as wod_e2ed_submission_pb2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLqKE2xj-tHH"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "Visit the [Waymo Open Dataset Website](https://waymo.com/open/) to download the\n",
        "full dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil du -s gs://waymo_open_dataset_end_to_end_camera_v_1_0_0/val_* #check bucket size"
      ],
      "metadata": {
        "id": "xzyKaHk5BH49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp gs://waymo_open_dataset_end_to_end_camera_v_1_0_0/val_* /content/dataset\n",
        "#downloading the valiation dataset cause its the smallest of the three"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MyXwGOrDBM8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgTIikH9Bhv6"
      },
      "source": [
        "Initialize dataset object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV_m-oc2Bbsn",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "DATASET_FOLDER = '/content/dataset'\n",
        "FILENAME_PATTERN = os.path.join(DATASET_FOLDER, 'val_*.tfrecord-*')\n",
        "\n",
        "# Match the files\n",
        "filenames = tf.io.matching_files(FILENAME_PATTERN)\n",
        "dataset = tf.data.TFRecordDataset(filenames, compression_type='')\n",
        "dataset_iter = dataset.as_numpy_iterator()\n",
        "print(filenames)\n",
        "print(\"______________\")\n",
        "print(dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaXCcjcVP60q"
      },
      "source": [
        "## Visualizing the future trajectories on image\n",
        "In this tutorial, we will visualize a single camera image and project the trajectory on the three front cameras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLv0k_XlTULt"
      },
      "outputs": [],
      "source": [
        "def return_front3_cameras(data: wod_e2ed_pb2.E2EDFrame):\n",
        "  \"\"\"Return the front_left, front, and front_right cameras as a list of images\"\"\"\n",
        "  image_list = []\n",
        "  calibration_list = []\n",
        "  # CameraName Enum reference:\n",
        "  # https://github.com/waymo-research/waymo-open-dataset/blob/5f8a1cd42491210e7de629b6f8fc09b65e0cbe99/src/waymo_open_dataset/dataset.proto#L50\n",
        "  order = [2, 1, 3]\n",
        "  for camera_name in order:\n",
        "    for index, image_content in enumerate(data.frame.images):\n",
        "      if image_content.name == camera_name:\n",
        "        # Decode the raw image string and convert to numpy type.\n",
        "        calibration = data.frame.context.camera_calibrations[index]\n",
        "        image = tf.io.decode_image(image_content.image).numpy()\n",
        "        image_list.append(image)\n",
        "        calibration_list.append(calibration)\n",
        "        break\n",
        "\n",
        "  return image_list, calibration_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yfg6ceNTJwq"
      },
      "source": [
        "Visualize the front 3 cameras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFePW7Y5WZik"
      },
      "outputs": [],
      "source": [
        "bytes_example = next(dataset_iter)\n",
        "data = wod_e2ed_pb2.E2EDFrame()\n",
        "data.ParseFromString(bytes_example)\n",
        "\n",
        "front3_camera_image_list, front3_camera_calibration_list = return_front3_cameras(data)\n",
        "concatenated_image = np.concatenate(front3_camera_image_list, axis=1)\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(concatenated_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV6sHoX1w2-k"
      },
      "outputs": [],
      "source": [
        "def project_vehicle_to_image(vehicle_pose, calibration, points):\n",
        "  \"\"\"Projects from vehicle coordinate system to image with global shutter.\n",
        "\n",
        "  Arguments:\n",
        "    vehicle_pose: Vehicle pose transform from vehicle into world coordinate\n",
        "      system.\n",
        "    calibration: Camera calibration details (including intrinsics/extrinsics).\n",
        "    points: Points to project of shape [N, 3] in vehicle coordinate system.\n",
        "\n",
        "  Returns:\n",
        "    Array of shape [N, 3], with the latter dimension composed of (u, v, ok).\n",
        "  \"\"\"\n",
        "  # Transform points from vehicle to world coordinate system (can be\n",
        "  # vectorized).\n",
        "  pose_matrix = np.array(vehicle_pose.transform).reshape(4, 4)\n",
        "  world_points = np.zeros_like(points)\n",
        "  for i, point in enumerate(points):\n",
        "    cx, cy, cz, _ = np.matmul(pose_matrix, [*point, 1])\n",
        "    world_points[i] = (cx, cy, cz)\n",
        "\n",
        "  # Populate camera image metadata. Velocity and latency stats are filled with\n",
        "  # zeroes.\n",
        "  extrinsic = tf.reshape(\n",
        "      tf.constant(list(calibration.extrinsic.transform), dtype=tf.float32),\n",
        "      [4, 4])\n",
        "  intrinsic = tf.constant(list(calibration.intrinsic), dtype=tf.float32)\n",
        "  metadata = tf.constant([\n",
        "      calibration.width,\n",
        "      calibration.height,\n",
        "      open_dataset.CameraCalibration.GLOBAL_SHUTTER,\n",
        "  ],\n",
        "                         dtype=tf.int32)\n",
        "  camera_image_metadata = list(vehicle_pose.transform) + [0.0] * 10\n",
        "\n",
        "  # Perform projection and return projected image coordinates (u, v, ok).\n",
        "  return py_camera_model_ops.world_to_image(extrinsic, intrinsic, metadata,\n",
        "                                            camera_image_metadata,\n",
        "                                            world_points).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAgH_pmoxrJo"
      },
      "outputs": [],
      "source": [
        "def draw_points_on_image(image, points, size):\n",
        "  \"\"\"Draws points on an image.\n",
        "\n",
        "  Args:\n",
        "    image: The image to draw on.\n",
        "    points: A numpy array of shape (N, 2) representing the points to draw.\n",
        "  \"\"\"\n",
        "  for point in points:\n",
        "    cv2.circle(image, (int(point[0]), int(point[1])), size, (255, 0, 0), -1)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL0uiCL1G4y6"
      },
      "source": [
        "Extract the ego vehicle's future trajectory and reshape to (N, 3) matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go_GRAlVpQJ6"
      },
      "outputs": [],
      "source": [
        "future_waypoints_matrix = np.stack([data.future_states.pos_x, data.future_states.pos_y, data.future_states.pos_z], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrEuJN4Lm5-7"
      },
      "source": [
        "The pose is always an identity matrix as we already convert world coordinates to vehicle coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqpRRhoGjbGb"
      },
      "outputs": [],
      "source": [
        "vehicle_pose = data.frame.images[0].pose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tucgPv_UoDTp"
      },
      "source": [
        "We convert the ego vehicle's future waypoints to camera space and draw on camera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKzDz0NLezyG"
      },
      "outputs": [],
      "source": [
        "images_with_drawn_points = []\n",
        "for i in range(len(front3_camera_calibration_list)):\n",
        "  waypoints_camera_space = project_vehicle_to_image(vehicle_pose, front3_camera_calibration_list[i], future_waypoints_matrix)\n",
        "  images_with_drawn_points.append(draw_points_on_image(front3_camera_image_list[i], waypoints_camera_space, size=15))\n",
        "concatenated_image = np.concatenate(images_with_drawn_points, axis=1)\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(concatenated_image)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}